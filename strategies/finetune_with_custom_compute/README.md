# ğŸ–¥ï¸ Finetuning with Custom Compute Strategy

> **Strategy #W of 10**: Slash Training Costs by 70% Using Spot Instances, Consumer GPUs, and Smart Resource Management

## ğŸ“‹ Executive Summary

Most companies overpay for LLM finetuning by using expensive cloud services or managed platforms. We show you how to train models using **consumer GPUs, spot instances, and custom compute** at a fraction of the cost.

### The Savings

| Approach | Cost for 7B Model | Time | Our Rating |
|----------|------------------|------|------------|
| **Managed Platform** (Replicate, Modal) | $150-300 | 8 hours | âŒ Expensive |
| **Cloud GPU** (AWS/GCP/Azure on-demand) | $80-120 | 8 hours | âš ï¸ Costly |
| **RunPod Spot** | $15-25 | 8 hours | âœ… Good |
| **Consumer GPU** (RTX 4090 local) | $0 (owned) | 10 hours | âœ… Best |
| **Hybrid** (Spot + optimizations) | $8-12 | 6 hours | â­ Optimal |

---

## ğŸ¯ The Problem

### Companies Are Overpaying for Training

```
Typical Scenario: Finetuning Llama-2-7B

Using Managed Platform (e.g., Replicate):
â”œâ”€ Platform fee: $200
â”œâ”€ Compute markup: 3-5x over bare metal
â”œâ”€ Limited customization
â”œâ”€ Vendor lock-in
â””â”€ Total: $200-300 per training run

Your Reality:
â”œâ”€ Need to iterate 10-20 times (experimenting)
â”œâ”€ Cost: $2,000-6,000 per project
â”œâ”€ Budget constraints kill innovation
â””â”€ "AI is too expensive" mindset
```

### Why This Happens

```
âŒ Problem 1: Cloud On-Demand Pricing
   AWS/GCP A100: $2-4/hour
   Training time: 40 hours
   Cost per run: $80-160

âŒ Problem 2: Don't Know About Spot Instances  
   Same GPU on spot: $0.50-1.00/hour
   Most teams aren't aware or don't use them
   
âŒ Problem 3: Over-Provisioned Resources
   Using A100 when RTX 4090 is sufficient
   Training on full precision when FP16 works
   Not using quantization or LoRA

âŒ Problem 4: Poor Resource Utilization
   GPU sitting idle during data loading
   No mixed precision training
   Inefficient batch sizes
   Single GPU when multi-GPU is available
```

---

## ğŸ’¡ What We Do Differently

### Our Custom Compute Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Smart Resource Selection                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  Step 1: Match GPU to Task                            â”‚
â”‚  â”œâ”€ 7B model? â†’ RTX 4090 (24GB) âœ…                    â”‚
â”‚  â”œâ”€ 13B model? â†’ A6000 (48GB) âœ…                      â”‚
â”‚  â””â”€ 70B model? â†’ 2x A100 (80GB) âœ…                    â”‚
â”‚                                                        â”‚
â”‚  Step 2: Use Spot Instances                           â”‚
â”‚  â”œâ”€ 70-80% cheaper than on-demand                     â”‚
â”‚  â”œâ”€ Checkpointing handles interruptions               â”‚
â”‚  â””â”€ Auto-resume on new instance                       â”‚
â”‚                                                        â”‚
â”‚  Step 3: Optimize Training                            â”‚
â”‚  â”œâ”€ LoRA instead of full finetuning                   â”‚
â”‚  â”œâ”€ Mixed precision (FP16/BF16)                       â”‚
â”‚  â”œâ”€ Gradient accumulation                             â”‚
â”‚  â””â”€ Efficient data loading                            â”‚
â”‚                                                        â”‚
â”‚  Step 4: Multi-GPU When Needed                        â”‚
â”‚  â”œâ”€ Cheaper dual RTX 4090 vs single A100              â”‚
â”‚  â”œâ”€ Parallel data loading                             â”‚
â”‚  â””â”€ Faster training time                              â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: Same quality, 70-90% lower cost
```

---

## ğŸ”¬ How We Do It

### 1. GPU Selection Matrix

```python
"""
Choose the right GPU for your model
"""

gpu_recommendations = {
    "llama-7b": {
        "minimum": "RTX 3090 (24GB)",
        "recommended": "RTX 4090 (24GB)",
        "cost_per_hour": "$0.50",
        "training_time": "8-10 hours",
        "total_cost": "$4-5",
        "use_lora": True  # Essential for 24GB
    },
    
    "llama-13b": {
        "minimum": "RTX A6000 (48GB)", 
        "recommended": "A6000 (48GB)",
        "cost_per_hour": "$0.80",
        "training_time": "12-16 hours",
        "total_cost": "$10-13",
        "use_lora": True
    },
    
    "llama-70b": {
        "minimum": "2x A100 (80GB)",
        "recommended": "4x A100 (80GB) or 8x RTX 4090",
        "cost_per_hour": "$2.50-4.00",
        "training_time": "24-40 hours",
        "total_cost": "$60-160",
        "use_lora": True,
        "use_deepspeed": True
    }
}

def recommend_gpu(model_name, budget):
    """Get GPU recommendation based on model and budget"""
    config = gpu_recommendations.get(model_name, {})
    
    if budget == "minimal":
        return config["minimum"]
    elif budget == "optimal":
        return config["recommended"]
```

### 2. Spot Instance Strategy

```python
"""
Use spot instances with automatic checkpointing
"""

class SpotInstanceTrainer:
    def __init__(self, model_name, checkpoint_dir="./checkpoints"):
        self.model_name = model_name
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_interval = 500  # Save every 500 steps
        
    def setup_training(self):
        """
        Configure training for spot instance interruptions
        """
        training_args = TrainingArguments(
            output_dir=self.checkpoint_dir,
            
            # Aggressive checkpointing for spot instances
            save_strategy="steps",
            save_steps=self.checkpoint_interval,
            save_total_limit=3,  # Keep last 3 checkpoints
            
            # Auto-resume from checkpoint
            resume_from_checkpoint=True,
            
            # Push to cloud storage (S3/GCS) regularly
            push_to_hub=True,  # Or use S3
            hub_strategy="checkpoint",
            
            # Enable all optimizations
            fp16=True,
            gradient_checkpointing=True,
            gradient_accumulation_steps=4,
        )
        
        return training_args
    
    def train_with_spot(self):
        """
        Train with automatic recovery from spot interruptions
        """
        print("ğŸ¯ Training on spot instance with auto-recovery...")
        
        try:
            # Normal training
            trainer.train(resume_from_checkpoint=True)
            
        except Exception as e:
            if "spot instance interrupted" in str(e):
                print("âš ï¸ Spot interrupted! Checkpoint saved.")
                print("ğŸ’¾ Resume training on new instance with:")
                print(f"   trainer.train(resume_from_checkpoint='{self.checkpoint_dir}')")
            else:
                raise e

# Usage
trainer_config = SpotInstanceTrainer("llama-2-7b")
args = trainer_config.setup_training()

# Training will auto-save and can resume after interruption
```

### 3. LoRA for Memory Efficiency

```python
"""
Use LoRA to train on consumer GPUs
"""

from peft import LoraConfig, get_peft_model, TaskType

def setup_lora_training(base_model, target_modules=None):
    """
    Configure LoRA for efficient training
    """
    # LoRA configuration
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,  # LoRA rank (higher = more parameters, but more memory)
        lora_alpha=32,  # Scaling factor
        lora_dropout=0.05,
        target_modules=target_modules or [
            "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
            "gate_proj", "up_proj", "down_proj"  # FFN
        ],
        bias="none",
    )
    
    # Apply LoRA to model
    model = get_peft_model(base_model, lora_config)
    
    # Print trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"ğŸ’¡ Trainable params: {trainable_params:,}")
    print(f"ğŸ“Š Total params: {total_params:,}")
    print(f"ğŸ¯ Percentage: {100 * trainable_params / total_params:.2f}%")
    
    return model

# Example: Llama-2-7B with LoRA
# Full finetuning: 7B parameters (28GB memory)
# LoRA finetuning: 4.2M parameters (8GB memory)
# Reduction: 99.94% fewer parameters!
```

### 4. Optimization Stack

```python
"""
Complete optimization for custom compute
"""

from transformers import TrainingArguments

def get_optimized_training_args(
    model_size="7b",
    gpu_memory="24gb",
    use_spot=True
):
    """
    Generate optimized training configuration
    """
    
    # Base configuration
    config = {
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 16,  # Effective batch size: 16
        "num_train_epochs": 3,
        "learning_rate": 2e-4,
        
        # Memory optimizations
        "fp16": True,  # Use mixed precision
        "gradient_checkpointing": True,  # Trade compute for memory
        "optim": "paged_adamw_8bit",  # 8-bit optimizer
        
        # Speed optimizations
        "dataloader_num_workers": 4,
        "dataloader_pin_memory": True,
        "ddp_find_unused_parameters": False,
        
        # Logging
        "logging_steps": 10,
        "evaluation_strategy": "steps",
        "eval_steps": 100,
    }
    
    # Adjust for GPU memory
    if gpu_memory == "24gb":
        config["per_device_train_batch_size"] = 1
        config["gradient_accumulation_steps"] = 16
    elif gpu_memory == "48gb":
        config["per_device_train_batch_size"] = 2
        config["gradient_accumulation_steps"] = 8
    
    # Spot instance settings
    if use_spot:
        config.update({
            "save_strategy": "steps",
            "save_steps": 500,
            "save_total_limit": 3,
            "load_best_model_at_end": True,
        })
    
    return TrainingArguments(**config)

# Usage
training_args = get_optimized_training_args(
    model_size="7b",
    gpu_memory="24gb", 
    use_spot=True
)
```

### 5. Multi-GPU Cost Optimization

```python
"""
Use multiple cheap GPUs instead of one expensive GPU
"""

# Option 1: Single A100 (80GB)
single_a100 = {
    "gpu": "1x A100 80GB",
    "cost_per_hour": "$2.50",
    "training_time": "10 hours",
    "total_cost": "$25"
}

# Option 2: Dual RTX 4090 (48GB total)
dual_4090 = {
    "gpu": "2x RTX 4090 24GB",
    "cost_per_hour": "$1.00",  # $0.50 each
    "training_time": "7 hours",  # Faster with parallelism
    "total_cost": "$7"
}

# Savings: $25 â†’ $7 (72% cheaper!)

# Implementation with DeepSpeed
deepspeed_config = {
    "train_batch_size": 16,
    "gradient_accumulation_steps": 4,
    "fp16": {"enabled": True},
    "zero_optimization": {
        "stage": 2,  # Partition optimizer states
        "offload_optimizer": {"device": "cpu"},  # Offload to CPU
    }
}

# Launch training with multiple GPUs
# deepspeed --num_gpus=2 train.py --deepspeed deepspeed_config.json
```

---

## ğŸ—ï¸ Platform Comparison

### Where to Train

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Platform Comparison (7B Model Training)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ ğŸ† RunPod (Spot)                                        â”‚
â”‚ â”œâ”€ Cost: $8-15                                          â”‚
â”‚ â”œâ”€ Setup: 5 minutes                                     â”‚
â”‚ â”œâ”€ Flexibility: High                                    â”‚
â”‚ â””â”€ Best for: Most teams                                 â”‚
â”‚                                                         â”‚
â”‚ âœ… Vast.ai (Spot)                                       â”‚
â”‚ â”œâ”€ Cost: $5-12                                          â”‚
â”‚ â”œâ”€ Setup: 10 minutes                                    â”‚
â”‚ â”œâ”€ Flexibility: Very high                               â”‚
â”‚ â””â”€ Best for: Lowest cost priority                       â”‚
â”‚                                                         â”‚
â”‚ âœ… Lambda Labs                                          â”‚
â”‚ â”œâ”€ Cost: $15-25                                         â”‚
â”‚ â”œâ”€ Setup: Instant                                       â”‚
â”‚ â”œâ”€ Flexibility: Medium                                  â”‚
â”‚ â””â”€ Best for: Reliability priority                       â”‚
â”‚                                                         â”‚
â”‚ âš ï¸ AWS/GCP/Azure (On-demand)                            â”‚
â”‚ â”œâ”€ Cost: $80-120                                        â”‚
â”‚ â”œâ”€ Setup: 30 minutes                                    â”‚
â”‚ â”œâ”€ Flexibility: Very high                               â”‚
â”‚ â””â”€ Best for: Enterprise with credits                    â”‚
â”‚                                                         â”‚
â”‚ âŒ Managed Platforms (Replicate, Modal)                 â”‚
â”‚ â”œâ”€ Cost: $150-300                                       â”‚
â”‚ â”œâ”€ Setup: Instant                                       â”‚
â”‚ â”œâ”€ Flexibility: Low                                     â”‚
â”‚ â””â”€ Best for: Quick demos only                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Local vs Cloud Decision

```python
"""
When to train locally vs cloud
"""

def should_use_local(gpu_owned, model_size, iterations):
    """
    Decide between local and cloud training
    """
    scenarios = {
        "have_rtx_4090_train_7b_once": {
            "decision": "LOCAL",
            "reason": "Free, one-time job",
            "cost": "$0"
        },
        
        "have_rtx_4090_train_7b_10x": {
            "decision": "LOCAL", 
            "reason": "Free vs $80 cloud",
            "cost": "$0 vs $80"
        },
        
        "no_gpu_train_7b_once": {
            "decision": "CLOUD_SPOT",
            "reason": "Cheap one-time ($10 vs $2000 GPU)",
            "cost": "$10"
        },
        
        "no_gpu_train_70b": {
            "decision": "CLOUD_SPOT",
            "reason": "Can't afford 8x A100 locally",
            "cost": "$60-100"
        },
        
        "have_rtx_3090_train_13b": {
            "decision": "CLOUD_SPOT",
            "reason": "Need 48GB, only have 24GB",
            "cost": "$15-20"
        }
    }
    
    return scenarios

# Key insight: Use local if you have the hardware, 
# use cloud spot for everything else
```

---

## ğŸ’° Real Cost Breakdown

### Case Study: Training Llama-2-7B for Customer Support

```
Project: Finetune Llama-2-7B on 10K customer support conversations
Iterations: 5 (experimenting with hyperparameters)
Total training runs: 5

Option 1: Managed Platform (Replicate)
â”œâ”€ Cost per run: $250
â”œâ”€ Total: 5 Ã— $250 = $1,250
â””â”€ Time: 40 hours total

Option 2: AWS On-Demand (A100)
â”œâ”€ Cost per run: $100 (50 hours Ã— $2/hr)
â”œâ”€ Total: 5 Ã— $100 = $500
â””â”€ Time: 50 hours total

Option 3: RunPod Spot (RTX 4090) â­
â”œâ”€ Cost per run: $10 (20 hours Ã— $0.50/hr)
â”œâ”€ Total: 5 Ã— $10 = $50
â””â”€ Time: 50 hours total (slightly slower)

Option 4: Local RTX 4090 (if owned) ğŸ†
â”œâ”€ Cost per run: $0 (free)
â”œâ”€ Total: $0
â””â”€ Time: 60 hours total (no parallelism)

Savings Comparison:
â”œâ”€ Replicate â†’ RunPod: $1,250 â†’ $50 (96% savings!)
â”œâ”€ AWS â†’ RunPod: $500 â†’ $50 (90% savings!)
â”œâ”€ RunPod â†’ Local: $50 â†’ $0 (100% savings!)
```

### Annual Savings

```
Scenario: ML team training 2 models per month

Managed Platform Annual Cost:
â”œâ”€ $250 per model Ã— 24 models/year
â””â”€ Total: $6,000/year

Custom Compute Annual Cost:
â”œâ”€ $10 per model Ã— 24 models/year
â””â”€ Total: $240/year

Annual Savings: $5,760 (96% reduction)

Break-even Point for Buying GPU:
â”œâ”€ RTX 4090: $1,600
â”œâ”€ Pays for itself in: 3.3 months
â””â”€ After that: Everything is free!
```

---

## ğŸš€ Quick Start Guide

### Step 1: Choose Your Platform

```bash
# RunPod (Recommended)
# 1. Go to runpod.io
# 2. Select "GPU Pods" â†’ "Spot"
# 3. Choose RTX 4090 or A6000
# 4. Select PyTorch template
# 5. SSH into instance

# Vast.ai (Cheapest)
# 1. Go to vast.ai
# 2. Search for RTX 4090 or A6000
# 3. Sort by price
# 4. Rent and SSH in
```

### Step 2: Setup Environment

```bash
# Quick setup script
pip install torch transformers accelerate peft bitsandbytes

# Verify GPU
nvidia-smi

# Clone training repo
git clone <your-training-repo>
cd training
```

### Step 3: Configure Training

```python
# config.yaml
model_name: "meta-llama/Llama-2-7b-hf"
dataset: "your-dataset"

# LoRA settings (memory efficient)
use_lora: true
lora_r: 16
lora_alpha: 32

# Training settings
batch_size: 1
gradient_accumulation: 16
epochs: 3
learning_rate: 2e-4

# Optimizations
fp16: true
gradient_checkpointing: true
optimizer: "paged_adamw_8bit"

# Spot instance settings
save_steps: 500
checkpoint_dir: "./checkpoints"
```

### Step 4: Start Training

```bash
# Single GPU training
python train.py --config config.yaml

# Multi-GPU training (if available)
accelerate launch --num_processes=2 train.py --config config.yaml

# Monitor progress
watch -n 1 nvidia-smi
```

### Step 5: Handle Spot Interruptions

```bash
# If spot instance interrupted, simply resume:
python train.py --config config.yaml --resume_from_checkpoint ./checkpoints/checkpoint-1000

# Auto-resume script (recommended)
while true; do
    python train.py --config config.yaml --resume_from_checkpoint auto || break
    sleep 5
done
```

---

## ğŸ“Š Optimization Checklist

### Before Training

```
âœ… Chose right GPU for model size
âœ… Using spot instances (not on-demand)
âœ… LoRA enabled for 7B-13B models
âœ… Mixed precision (FP16) enabled
âœ… Gradient checkpointing enabled
âœ… Batch size optimized for GPU memory
âœ… Gradient accumulation configured
âœ… Checkpoint saving every 500 steps
âœ… Data preprocessing complete
âœ… Test run on small dataset first
```

### During Training

```
âœ… Monitor GPU utilization (should be >90%)
âœ… Check memory usage (should be near max)
âœ… Watch loss curve (should decrease)
âœ… Verify checkpoints saving correctly
âœ… Track training speed (tokens/sec)
```

### Common Issues & Fixes

```
Issue: Out of Memory (OOM)
Fix:
â”œâ”€ Reduce batch size to 1
â”œâ”€ Enable gradient checkpointing
â”œâ”€ Use LoRA (if not already)
â””â”€ Try quantized optimizer

Issue: Slow Training
Fix:
â”œâ”€ Increase batch size (if memory allows)
â”œâ”€ Use multiple GPUs
â”œâ”€ Check dataloader workers (4-8)
â””â”€ Profile with torch.profiler

Issue: Spot Instance Interrupted
Fix:
â”œâ”€ Auto-resume from checkpoint
â”œâ”€ Use cheaper GPU if available
â””â”€ Run during off-peak hours
```

---

## ğŸ¯ Best Practices

### Cost Optimization

```
1. Always use spot instances
   - 70-80% cheaper than on-demand
   - Interruptions are rare
   - Checkpointing handles them

2. Right-size your GPU
   - Don't use A100 for 7B models
   - RTX 4090 is sufficient
   
3. Use LoRA for everything
   - 99% fewer parameters
   - 4x less memory
   - Minimal accuracy loss

4. Buy GPU if training regularly
   - RTX 4090: $1,600
   - Pays for itself in months
   - Then everything is free
```

### Performance Optimization

```
1. Enable all memory optimizations
   âœ… FP16/BF16 mixed precision
   âœ… Gradient checkpointing
   âœ… 8-bit optimizer
   âœ… Gradient accumulation

2. Optimize data loading
   âœ… Preprocess data once
   âœ… Use fast dataloader
   âœ… Pin memory
   âœ… Multiple workers

3. Monitor and adjust
   âœ… Watch GPU utilization
   âœ… Increase batch size if possible
   âœ… Profile slow sections
```

---

## ğŸ’¼ Service Packages

### Package 1: Training Setup & Optimization
**Duration:** 1 week | **Price:** $3,000 - $5,000

```
âœ… GPU selection consultation
âœ… Training pipeline setup
âœ… Optimization implementation
âœ… Spot instance configuration
âœ… Cost analysis & recommendations

Deliverable: Optimized training setup + documentation
Savings: 70-90% on training costs
```

### Package 2: Managed Training Service
**Duration:** Ongoing | **Price:** $500/month + compute costs

```
âœ… We handle all training
âœ… Spot instance management
âœ… Automatic checkpointing
âœ… Progress monitoring
âœ… Model delivery

Deliverable: Trained models on demand
Your cost: Only actual compute (at our discounted rates)
```

---

## ğŸ“ Get Started

**Ready to cut your training costs by 70-90%?**

ğŸ“§ Email: your-email@company.com  
ğŸ’¼ Schedule: [Calendar Link]

**Free Consultation:**
- Analyze your training needs
- Calculate potential savings  
- Recommend optimal setup

---

<div align="center">

**Strategy #W: Custom Compute Training**  
*70-90% Cost Reduction | Same Quality | Full Control*

ğŸ’° Train Smart | âš¡ Save Big | ğŸš€ Stay Flexible

</div>
