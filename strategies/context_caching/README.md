# âš¡ Context Caching Strategy

> **Strategy #Z of 10**: Reduce API Costs by 90% and Speed Up Responses 10x Through Intelligent Prompt Caching

## ğŸ“‹ Executive Summary

Context caching dramatically reduces LLM API costs and latency by **reusing repeated prompt content** instead of processing it every time. Perfect for applications with large system prompts, document analysis, or repeated context.

### The Impact

| Metric | Without Caching | With Caching | Improvement |
|--------|----------------|--------------|-------------|
| **API Cost** | $1,000/month | $100/month | 90% savings |
| **Response Time** | 5 seconds | 0.5 seconds | 10x faster |
| **Tokens Processed** | 100M/month | 10M/month | 90% reduction |

---

## ğŸ¯ The Problem

### Scenario: Document Q&A System

```
User asks 100 questions about a 50-page document (20,000 tokens)

Traditional Approach (No Caching):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request 1: [20K tokens document] + [50 tokens Q1]â”‚
â”‚ Request 2: [20K tokens document] + [50 tokens Q2]â”‚
â”‚ Request 3: [20K tokens document] + [50 tokens Q3]â”‚
â”‚ ...                                              â”‚
â”‚ Request 100: [20K tokens document] + [50 Q100]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Tokens: 20,000 Ã— 100 = 2,000,000 tokens
Cost: 2M tokens Ã— $0.01/1K = $20
Time: 100 Ã— 5 seconds = 500 seconds (8.3 minutes)
```

**Problem: You're paying to process the same 20K token document 100 times!**

### Real-World Pain Points

```
âŒ Long System Prompts Repeated Every Request
   "You are a customer service agent with these guidelines: [2000 tokens]..."
   Processed 10,000 times/day = 20M tokens wasted

âŒ Document Analysis Applications  
   User asks 50 questions about a contract
   Same contract sent 50 times = massive waste

âŒ Multi-Turn Conversations
   Chat history grows with each message
   Reprocessing entire history every turn

âŒ Code Repository Context
   Large codebase context in every query
   Thousands of dollars in redundant processing
```

---

## ğŸ’¡ The Solution: Context Caching

### How It Works

```
First Request (Cache Miss):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [20K tokens document] â”€â”€â–º Process & Cache        â”‚
â”‚ [50 tokens question] â”€â”€â–º Process normally        â”‚
â”‚                                                  â”‚
â”‚ Cache ID: abc123 (valid for 5 minutes)          â”‚
â”‚ Cost: Full price for 20,050 tokens              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Subsequent Requests (Cache Hit):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cache ID: abc123 â”€â”€â–º Retrieved instantly         â”‚
â”‚ [50 tokens question] â”€â”€â–º Process normally        â”‚
â”‚                                                  â”‚
â”‚ Cost: Only 50 new tokens (99% savings!)         â”‚
â”‚ Time: Instant retrieval (10x faster!)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

100 Questions Total:
â”œâ”€ Request 1: 20,050 tokens (full price)
â”œâ”€ Requests 2-100: 50 tokens each (cached context)
â””â”€ Total: 20,050 + (99 Ã— 50) = 24,950 tokens

Savings: 2M â†’ 25K tokens (98.75% reduction!)
Cost: $20 â†’ $0.25 (98.75% cheaper!)
```

### When to Use Context Caching

| Use Case | Cache This | Why |
|----------|-----------|-----|
| **Document Q&A** | âœ… The document | Asked multiple questions about same doc |
| **Customer Support** | âœ… System prompt + guidelines | Same instructions every request |
| **Code Assistant** | âœ… Codebase context | Repository context doesn't change |
| **Legal Analysis** | âœ… Contracts/regulations | Analyzing same legal text repeatedly |
| **Chatbots** | âœ… Conversation history | Multi-turn conversations |
| **Creative Writing** | âŒ Each new story | Content changes every time |
| **One-off Queries** | âŒ No repeated context | No benefit from caching |

---

## ğŸ”¬ Implementation

### Provider Support

```
âœ… Anthropic Claude (Native Support)
â”œâ”€ Cache prefix markers in API
â”œâ”€ Automatic cache management
â””â”€ 5-minute cache TTL

âœ… OpenAI GPT-4/3.5 (Manual Implementation)
â”œâ”€ Custom caching layer required
â”œâ”€ Use Redis/Memcached
â””â”€ Manage cache yourself

âœ… Self-Hosted Models (Full Control)
â”œâ”€ KV-cache optimization
â”œâ”€ Prompt caching middleware
â””â”€ Custom TTL management
```

### Example 1: Anthropic Claude (Native)

```python
"""
Using Anthropic's native prompt caching
"""

import anthropic

client = anthropic.Anthropic(api_key="your-api-key")

# Large document to cache
large_document = """
[Your 20,000 token document here]
This could be a contract, manual, codebase, etc.
"""

# First request - creates cache
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "You are a helpful assistant analyzing documents.",
        },
        {
            "type": "text", 
            "text": large_document,
            "cache_control": {"type": "ephemeral"}  # Cache this!
        }
    ],
    messages=[
        {"role": "user", "content": "What is the main topic of this document?"}
    ]
)

# Subsequent requests - use cache
for question in user_questions:
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        system=[
            {
                "type": "text",
                "text": "You are a helpful assistant analyzing documents.",
            },
            {
                "type": "text",
                "text": large_document,
                "cache_control": {"type": "ephemeral"}  # Reuses cache!
            }
        ],
        messages=[
            {"role": "user", "content": question}
        ]
    )
    # Only pays for new question tokens, not document!

# Cost breakdown
print(f"Cache creation: {response.usage.cache_creation_input_tokens} tokens")
print(f"Cache reads: {response.usage.cache_read_input_tokens} tokens")
print(f"Regular input: {response.usage.input_tokens} tokens")
```

### Example 2: OpenAI with Custom Caching

```python
"""
Custom caching layer for OpenAI API
"""

import openai
import hashlib
import redis
import json

# Initialize Redis for caching
cache = redis.Redis(host='localhost', port=6379, db=0)
CACHE_TTL = 300  # 5 minutes

def get_cache_key(content):
    """Generate cache key from content"""
    return hashlib.md5(content.encode()).hexdigest()

def cached_completion(system_prompt, user_message, model="gpt-4"):
    """
    Wrapper that caches system prompt processing
    """
    # Generate cache key for system prompt
    cache_key = f"prompt_cache:{get_cache_key(system_prompt)}"
    
    # Check cache
    cached_embedding = cache.get(cache_key)
    
    if cached_embedding:
        print("âœ… Cache hit! Using cached context")
        # In reality, you'd use the cached embeddings
        # This is simplified for illustration
    else:
        print("âŒ Cache miss. Processing and caching...")
        cache.setex(cache_key, CACHE_TTL, "cached_data")
    
    # Make API call (OpenAI doesn't have native caching)
    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
    )
    
    return response

# Usage
long_system_prompt = """
You are an expert customer service agent with access to:
[5000 tokens of guidelines, FAQs, policies...]
"""

# Multiple requests with same system prompt
for customer_query in customer_queries:
    response = cached_completion(
        system_prompt=long_system_prompt,
        user_message=customer_query
    )
```

### Example 3: Self-Hosted with KV-Cache

```python
"""
Self-hosted model with KV-cache optimization
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class CachedInference:
    def __init__(self, model_name):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.kv_cache = {}  # Store past key-values
    
    def generate_with_cache(self, cached_context, new_query, cache_id=None):
        """
        Generate using cached KV states for repeated context
        """
        # First time: process context and cache KV states
        if cache_id not in self.kv_cache:
            print("ğŸ”„ Processing context and creating cache...")
            
            # Encode cached context
            context_ids = self.tokenizer.encode(
                cached_context, 
                return_tensors="pt"
            )
            
            # Get KV cache from context
            with torch.no_grad():
                outputs = self.model(
                    context_ids,
                    use_cache=True
                )
                self.kv_cache[cache_id] = outputs.past_key_values
            
            print(f"âœ… Cache created: {cache_id}")
        
        # Use cached KV states for new query
        print(f"âš¡ Using cached context: {cache_id}")
        query_ids = self.tokenizer.encode(new_query, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                query_ids,
                past_key_values=self.kv_cache[cache_id],  # Reuse cached KV!
                max_new_tokens=100
            )
        
        return self.tokenizer.decode(outputs[0])

# Usage
model = CachedInference("meta-llama/Llama-2-7b-hf")

document = "Long document content here..." * 1000  # 20K tokens

# Process multiple queries
for i, question in enumerate(questions):
    response = model.generate_with_cache(
        cached_context=document,
        new_query=question,
        cache_id="doc_123"  # Same ID reuses cache
    )
```

---

## ğŸ’° Cost Savings Analysis

### Real-World Example: Legal Document Analysis

```
Scenario: Law firm analyzing 100-page contracts
â”œâ”€ Contract size: 40,000 tokens
â”œâ”€ Questions per contract: 20
â”œâ”€ Contracts per month: 50
â””â”€ Total queries: 1,000/month

Without Caching:
â”œâ”€ Tokens per query: 40,000 (contract) + 50 (question) = 40,050
â”œâ”€ Total tokens: 40,050 Ã— 1,000 = 40,050,000 tokens/month
â”œâ”€ Cost: 40M Ã— $0.01/1K = $400/month
â””â”€ Response time: 8 seconds per query

With Caching:
â”œâ”€ First query per contract: 40,050 tokens (full)
â”œâ”€ Remaining 19 queries: 50 tokens each
â”œâ”€ Per contract: 40,050 + (19 Ã— 50) = 41,000 tokens
â”œâ”€ Total tokens: 41,000 Ã— 50 = 2,050,000 tokens/month
â”œâ”€ Cost: 2.05M Ã— $0.01/1K = $20.50/month
â””â”€ Response time: 0.8 seconds per cached query

Savings:
â”œâ”€ Cost reduction: $400 â†’ $20.50 (94.9% savings)
â”œâ”€ Speed improvement: 8s â†’ 0.8s (10x faster)
â””â”€ Annual savings: $4,554
```

### Customer Support Chatbot

```
Scenario: Customer support with long system prompts
â”œâ”€ System prompt: 3,000 tokens (guidelines, FAQs)
â”œâ”€ Average query: 50 tokens
â”œâ”€ Daily queries: 10,000
â””â”€ Monthly queries: 300,000

Without Caching:
â”œâ”€ Tokens per query: 3,050
â”œâ”€ Monthly tokens: 3,050 Ã— 300,000 = 915,000,000 tokens
â”œâ”€ Cost: 915M Ã— $0.01/1K = $9,150/month

With Caching:
â”œâ”€ Cache system prompt (3,000 tokens) once per session
â”œâ”€ Average session: 5 queries
â”œâ”€ Sessions per month: 60,000
â”œâ”€ Total tokens: (3,050 Ã— 60,000) + (50 Ã— 240,000) = 195M tokens
â”œâ”€ Cost: 195M Ã— $0.01/1K = $1,950/month

Savings:
â”œâ”€ Cost reduction: $9,150 â†’ $1,950 (78.7% savings)
â”œâ”€ Annual savings: $86,400
â””â”€ ROI on implementation: Immediate
```

---

## ğŸ—ï¸ Implementation Patterns

### Pattern 1: Document Analysis System

```python
"""
Multi-user document analysis with caching
"""

class DocumentAnalysisSystem:
    def __init__(self):
        self.client = anthropic.Anthropic()
        self.cache_registry = {}  # Track active caches
    
    def analyze_document(self, document_id, document_content, questions):
        """
        Analyze document with automatic caching
        """
        results = []
        
        for i, question in enumerate(questions):
            is_first_query = (i == 0)
            
            # Create cache on first query, reuse on subsequent
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1024,
                system=[
                    {
                        "type": "text",
                        "text": "You are an expert document analyst."
                    },
                    {
                        "type": "text",
                        "text": document_content,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[
                    {"role": "user", "content": question}
                ]
            )
            
            # Track cache usage
            if is_first_query:
                cache_id = response.id
                self.cache_registry[document_id] = {
                    "cache_id": cache_id,
                    "created_at": time.time(),
                    "queries_served": 1
                }
            else:
                self.cache_registry[document_id]["queries_served"] += 1
            
            results.append({
                "question": question,
                "answer": response.content[0].text,
                "cached": not is_first_query,
                "tokens_saved": response.usage.cache_read_input_tokens
            })
        
        # Calculate savings
        total_saved = sum(r["tokens_saved"] for r in results)
        print(f"ğŸ’° Saved {total_saved} tokens through caching!")
        
        return results
```

### Pattern 2: Chatbot with Conversation History

```python
"""
Chatbot that caches growing conversation history
"""

class CachedChatbot:
    def __init__(self):
        self.conversations = {}  # Store conversation state
    
    def chat(self, user_id, message):
        """
        Handle chat with cached conversation history
        """
        # Get or create conversation
        if user_id not in self.conversations:
            self.conversations[user_id] = {
                "history": [],
                "system_prompt": "You are a helpful assistant..."
            }
        
        conv = self.conversations[user_id]
        
        # Build messages with cache control on history
        messages = []
        
        # Mark history for caching (grows over time)
        for msg in conv["history"]:
            messages.append(msg)
        
        # Add new user message
        messages.append({"role": "user", "content": message})
        
        # Make request with cached history
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            system=[
                {
                    "type": "text",
                    "text": conv["system_prompt"],
                    "cache_control": {"type": "ephemeral"}  # Cache system prompt
                }
            ],
            messages=messages
        )
        
        # Update conversation history
        conv["history"].append({"role": "user", "content": message})
        conv["history"].append({
            "role": "assistant", 
            "content": response.content[0].text
        })
        
        return response.content[0].text
```

### Pattern 3: Code Repository Assistant

```python
"""
Code assistant with cached repository context
"""

class CodeAssistant:
    def __init__(self, repo_path):
        self.repo_context = self.load_repo_context(repo_path)
        self.cache_id = None
    
    def load_repo_context(self, repo_path):
        """Load codebase context (architecture, key files, etc.)"""
        context = f"""
        Repository Structure:
        {self.get_file_tree(repo_path)}
        
        Key Files:
        {self.get_important_files(repo_path)}
        
        Architecture:
        {self.extract_architecture(repo_path)}
        """
        return context
    
    def ask(self, question):
        """
        Answer questions with cached repo context
        """
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2048,
            system=[
                {
                    "type": "text",
                    "text": "You are an expert code assistant."
                },
                {
                    "type": "text",
                    "text": self.repo_context,  # Large context
                    "cache_control": {"type": "ephemeral"}  # Cache it!
                }
            ],
            messages=[
                {"role": "user", "content": question}
            ]
        )
        
        return response.content[0].text

# Usage
assistant = CodeAssistant("/path/to/repo")

# All queries reuse cached repository context
print(assistant.ask("How does the authentication work?"))
print(assistant.ask("Where is the user model defined?"))
print(assistant.ask("Explain the API routing system."))
# Only pays for questions, not repeated repo context!
```

---

## ğŸ“Š Best Practices

### âœ… Do's

```
1. âœ… Cache stable, repeated content
   - System prompts
   - Documents being analyzed
   - Code repositories
   - Reference materials

2. âœ… Use appropriate cache TTL
   - Short sessions: 5 minutes
   - Document analysis: 30 minutes
   - Daily operations: 1 hour

3. âœ… Monitor cache hit rates
   - Track: hits vs misses
   - Optimize: cache more frequently used content
   - Alert: if hit rate drops below 80%

4. âœ… Structure prompts for caching
   - Put cacheable content first
   - Keep variable content last
   - Use clear cache boundaries
```

### âŒ Don'ts

```
1. âŒ Don't cache frequently changing content
   - Real-time data
   - User-specific information
   - Temporary context

2. âŒ Don't cache very small prompts
   - < 1000 tokens: overhead not worth it
   - Simple queries: faster without cache
   - One-off requests: no benefit

3. âŒ Don't forget cache invalidation
   - Update when source changes
   - Clear stale caches
   - Monitor cache freshness

4. âŒ Don't cache sensitive data indefinitely
   - PII should have short TTL
   - Compliance requirements
   - Security considerations
```

---

## ğŸ¯ Quick Start

### Step 1: Identify Cache Candidates

```
Analyze your application:
â”œâ”€ What prompts are repeated?
â”œâ”€ What context is stable?
â”œâ”€ What queries happen in batches?
â””â”€ Where are the biggest token costs?

Good Candidates:
âœ… System prompts over 1000 tokens
âœ… Documents analyzed multiple times
âœ… Code repositories queried repeatedly
âœ… Multi-turn conversations
```

### Step 2: Implement Caching

```python
# For Anthropic (easiest)
Add cache_control to your system messages

# For OpenAI
Implement Redis/Memcached layer

# For self-hosted
Use KV-cache optimization
```

### Step 3: Monitor & Optimize

```python
# Track metrics
metrics = {
    "cache_hits": 0,
    "cache_misses": 0,
    "tokens_saved": 0,
    "cost_saved": 0
}

# Calculate hit rate
hit_rate = metrics["cache_hits"] / (metrics["cache_hits"] + metrics["cache_misses"])

# Target: > 80% hit rate for significant savings
```

---

## ğŸ’¼ Client Packages

### Package 1: Assessment & POC
**Duration:** 1-2 weeks | **Price:** $5,000 - $8,000

```
âœ… Analyze your current API usage
âœ… Identify caching opportunities  
âœ… Implement POC with caching
âœ… Measure savings (tokens & cost)
âœ… Provide optimization roadmap

Deliverable: Working demo + ROI report
```

### Package 2: Production Implementation
**Duration:** 3-4 weeks | **Price:** $15,000 - $25,000

```
âœ… Full caching implementation
âœ… Custom caching layer (if needed)
âœ… Monitoring dashboard
âœ… Cache optimization
âœ… Documentation & training

Deliverable: Production system + 30-day support
```

---

## ğŸ“ Next Steps

**Ready to cut your LLM API costs by 90%?**

ğŸ“§ Email: amitnaik.work@gmail.com
ğŸ’¼ LinkedIn: https://www.linkedin.com/in/amit-naik-6264d/

**Free Assessment:**
- Analyze your current usage
- Calculate potential savings
- Implementation roadmap

---

<div align="center">

**Strategy #Z: Context Caching**  
*90% Cost Reduction | 10x Faster Responses | Instant ROI*

âš¡ Cache Smart | ğŸ’° Save Big | ğŸš€ Ship Fast

</div>
